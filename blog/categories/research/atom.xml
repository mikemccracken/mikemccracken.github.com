<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: research | michael-mccracken.net]]></title>
  <link href="http://michael-mccracken.net/blog/categories/research/atom.xml" rel="self"/>
  <link href="http://michael-mccracken.net/"/>
  <updated>2012-08-09T00:03:35-07:00</updated>
  <id>http://michael-mccracken.net/</id>
  <author>
    <name><![CDATA[Michael McCracken]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Link: The Future of Computing Performance: Game Over or Next Level?]]></title>
    <link href="http://michael-mccracken.net/blog/2011/06/27/link-the-future-of-computing-performance-game-over-or-next-level/"/>
    <updated>2011-06-27T04:10:01-07:00</updated>
    <id>http://michael-mccracken.net/blog/2011/06/27/link-the-future-of-computing-performance-game-over-or-next-level</id>
    <content type="html"><![CDATA[<p>From the National Academies:</p>

<p><a href="http://nap.edu/catalog.php?record_id=12980">The Future of Computing Performance: Game Over or Next Level?</a>.</p>

<p>A nice, thorough explanation of the current challenges in computing performance, ranging from transistor-level power vs. speed problems, up to how to program the circuits we're likely to end up with.</p>

<p>Also includes a bonus reprint of two classic papers, Gordon Moore's "Cramming More Components onto Integrated Circuits" from 1965 - that's the paper you might expect it is, and Robert Dennard's "Design of Ion-Implanted MOSFET’s with Very Small Physical Dimensions", which I was less familiar with.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA["Squeezing a CS Research Idea"]]></title>
    <link href="http://michael-mccracken.net/blog/2011/05/27/squeezing-a-cs-research-idea/"/>
    <updated>2011-05-27T03:03:59-07:00</updated>
    <id>http://michael-mccracken.net/blog/2011/05/27/squeezing-a-cs-research-idea</id>
    <content type="html"><![CDATA[<p>An interesting post about how to evaluate a research idea. Basically, try to think about the bounds: what's the maximum impact? Includes some good rules of thumb, like pay attention to physical constraints like the speed of light :)</p>

<p><a href="http://blog.regehr.org/archives/537">Embedded in Academia : Squeezing a CS Research Idea</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Volatile and Decentralized: The death of Intel Labs and what it means for industrial research]]></title>
    <link href="http://michael-mccracken.net/blog/2011/04/05/volatile-and-decentralized-the-death-of-intel-labs-and-what-it-means-for-industrial-research/"/>
    <updated>2011-04-05T16:55:10-07:00</updated>
    <id>http://michael-mccracken.net/blog/2011/04/05/volatile-and-decentralized-the-death-of-intel-labs-and-what-it-means-for-industrial-research</id>
    <content type="html"><![CDATA[<p><a href="http://matt-welsh.blogspot.com/2011/04/death-of-intel-labs-and-what-it-means.html">Volatile and Decentralized: The death of Intel Labs and what it means for industrial research</a>.</p>

<p>Matt Welsh (ex Harvard Professor, now at Google) on the phasing out of Intel's "Lablets", which were a new kind of company-supported research center that was co-located with major universities. His blog post title notwithstanding, this doesn't seem to be about Intel moving away from research in general, just this particular model of heavy university interaction.</p>

<p>I don't want to comment directly on his post (see disclaimer below), but I'm posting this because there are a lot of interesting comments on his site, including some people sharing their views from inside other industrial labs. Welsh suggests that industrial research is on the wane and that Google's model of advanced development is the future, and others chime in to say that research is alive at many companies, some more directly focused on (read: paid for by) product groups than others.</p>

<p><strong>Update:</strong> See also the <a href="http://news.ycombinator.com/item?id=2405579">comments on the post at Hacker News</a>, in particular <a href="http://news.ycombinator.com/item?id=2407566">this long one by <code>NY_USA_Hacker</code></a>, who is continuing his (?) tradition of posting marathon irreverent comments about the value of a PhD, and advocating mathematics PhDs over computer science, even for tech folks.</p>

<p><em>The views expressed on this blog are my own and do not necessarily reflect the views of Oracle.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HPC blogs and news sites]]></title>
    <link href="http://michael-mccracken.net/blog/2007/05/04/hpc-blogs-and-news-sites/"/>
    <updated>2007-05-04T07:17:42-07:00</updated>
    <id>http://michael-mccracken.net/blog/2007/05/04/hpc-blogs-and-news-sites</id>
    <content type="html"><![CDATA[<p>I've always liked the programming-languages community website <a href="http://lambda-the-ultimate.org/">Lambda the Ultimate</a>, and recently I went looking for something similar for the High-Performance Computing community. I didn't find exactly that*, but I did find a few great resources for news about HPC and computing research policy:</p>

<p><a href="http://hpcwire.com">HPCWire</a> is a well-known news source for HPC. It has daily news updates, and occasional columns by guests from around the industry. Most of the news is press releases from companies and government labs, but it's nice to have a single place to check for them. However, there is apparently no RSS feed, and to get email updates, you have to buy a subscription. I haven't, but I do check back occasionally to read the columns.</p>

<p><a href="http://supercomputingonline.com">Supercomputing Online</a> is another professional news outlet, which reads a little less like a press release,
 seems to have more coverage of academic news, and does have an RSS feed.</p>

<p><a href="http://insideHPC.com/">insideHPC.com</a> is more of a weblog than either of the first two - John E. West covers news stories in brief with some added perspective and analysis. I like his approach, and <a href="http://insidehpc.com/2007/05/02/introducing-mike/">I've joined him</a> to help cover academic HPC issues, both computing research and issues affecting computational scientists.</p>

<p>One of many blogs at Sun is the <a href="http://blogs.sun.com/HPC/">HPC Watercooler</a>, covering Sun's HPC products and services, as well as some non-Sun related news. I've found it pretty interesting already, and I'd be interested to see weblogs from other HPC vendors.</p>

<p>Finally, a couple of blogs that are less directly related to HPC but still very relevant for computing researchers, are <a href="http://www.renci.org/blog/index.php">Dan Reed's weblog</a> at the Renaissance Computing Institute, and the <a href="http://www.cra.org/govaffairs/blog/">CRA Computing Research Policy Blog</a>, both of which cover computing research policy and funding issues that don't often show up in news coverage of either government or computing.</p>

<p><em>* - if anyone wants to start an LTU-alike site for HPC research, or point me to one, I'll sign up and contribute in an instant.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The TRIPS processor]]></title>
    <link href="http://michael-mccracken.net/blog/2007/04/25/the-trips-processor/"/>
    <updated>2007-04-25T04:01:48-07:00</updated>
    <id>http://michael-mccracken.net/blog/2007/04/25/the-trips-processor</id>
    <content type="html"><![CDATA[<p>The UT-Austin <a href="http://www.cs.utexas.edu/~trips/">TRIPS project</a> will be unveiling their new processor next Monday. (<a href="http://oea.cs.utexas.edu/articles/index2007/trips_unveiling07.html">event details</a>)</p>

<p>This is a pretty interesting attempt to get around the problems facing processor design today. Clock speeds have stalled, but the actual Moore's Law - the one about transistor count, not "speed" - is still going, so we have the problem of what to do with just a lot of copies of basically the same old chip?</p>

<p>A lot of answers you hear involve pushing that complexity up to the programmer, forcing more people to become parallel programmers. This is almost certain to happen at least a little, but let's hope we don't have to give up on the sequential programming model completely. If you think software is bad now…</p>

<p>The TRIPS processor is an example of another approach - placing more of the burden of finding and using parallelism onto the compiler and architecture, keeping programmers' heads above water. It's pretty exciting to see something this different make its way into actual silicon.</p>

<p>The basic idea is that instead of a single piece of control logic organizing the actions of multiple functional units, finding concurrency within a window of instructions using reordering, the TRIPS processor is distributed at the lowest level - each functional unit is a mini-processor (called a tile), and instructions executing on separate processor tiles communicate operands directly, not through a register file. Usually this is described as executing a graph of instructions instead of a single instruction at a time.</p>

<p>Current processors certainly don't just execute one instruction at a time, and they do plenty of moving instructions around, so I tend to see this explicit-data-graph description as just the far end of a spectrum that starts with old superscalar designs, continues through out-of-order processors and multithreaded architectures, and currently seems to end here.</p>

<p>A TRIPS processor can run four thread contexts at once, with an instruction window of 1024 instructions to reorder and 256 memory operations in flight at once. For comparison, the late '90s Tera MTA ran 128 threads at once (128 different program counters), and the 2003-vintage Cray X1 processors kept track of 512 memory operations at once. Just like TRIPS, each of those architectures required extensive compiler support for good performance.</p>

<p>A particularly interesting point is the fully partitioned L1 cache - meaning that there are multiple distributed L1 caches on the chip, so where your instructions are physically executing will be important for performance - if they're near the cache bank holding their operands, they will execute sooner.</p>

<p>The natural question when looking at a new and interesting architecture like this, especially one that promises a tera-op on a chip, is whether it will make its way to a laptop you can buy anytime soon. I have no idea if the UT team has any industry deals in the works, but I would bet against something like this becoming mainstream quickly - the fact that these architectures rely so much on a custom compiler with aggressive optimization means that a lot of dirty work is required to move existing software to it.</p>

<p>It will be interesting to follow this project and see how their actual hardware performs.</p>
]]></content>
  </entry>
  
</feed>
