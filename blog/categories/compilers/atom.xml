<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: compilers | michael-mccracken.net]]></title>
  <link href="http://mikemccracken.github.com/blog/categories/compilers/atom.xml" rel="self"/>
  <link href="http://mikemccracken.github.com/"/>
  <updated>2012-08-17T00:14:08-07:00</updated>
  <id>http://mikemccracken.github.com/</id>
  <author>
    <name><![CDATA[Michael McCracken]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The TRIPS processor]]></title>
    <link href="http://mikemccracken.github.com/blog/2007/04/25/the-trips-processor/"/>
    <updated>2007-04-25T04:01:48-07:00</updated>
    <id>http://mikemccracken.github.com/blog/2007/04/25/the-trips-processor</id>
    <content type="html"><![CDATA[<p>The UT-Austin <a href="http://www.cs.utexas.edu/~trips/">TRIPS project</a> will be unveiling their new processor next Monday. (<a href="http://oea.cs.utexas.edu/articles/index2007/trips_unveiling07.html">event details</a>)</p>

<p>This is a pretty interesting attempt to get around the problems facing processor design today. Clock speeds have stalled, but the actual Moore's Law - the one about transistor count, not "speed" - is still going, so we have the problem of what to do with just a lot of copies of basically the same old chip?</p>

<p>A lot of answers you hear involve pushing that complexity up to the programmer, forcing more people to become parallel programmers. This is almost certain to happen at least a little, but let's hope we don't have to give up on the sequential programming model completely. If you think software is bad nowâ€¦</p>

<p>The TRIPS processor is an example of another approach - placing more of the burden of finding and using parallelism onto the compiler and architecture, keeping programmers' heads above water. It's pretty exciting to see something this different make its way into actual silicon.</p>

<p>The basic idea is that instead of a single piece of control logic organizing the actions of multiple functional units, finding concurrency within a window of instructions using reordering, the TRIPS processor is distributed at the lowest level - each functional unit is a mini-processor (called a tile), and instructions executing on separate processor tiles communicate operands directly, not through a register file. Usually this is described as executing a graph of instructions instead of a single instruction at a time.</p>

<p>Current processors certainly don't just execute one instruction at a time, and they do plenty of moving instructions around, so I tend to see this explicit-data-graph description as just the far end of a spectrum that starts with old superscalar designs, continues through out-of-order processors and multithreaded architectures, and currently seems to end here.</p>

<p>A TRIPS processor can run four thread contexts at once, with an instruction window of 1024 instructions to reorder and 256 memory operations in flight at once. For comparison, the late '90s Tera MTA ran 128 threads at once (128 different program counters), and the 2003-vintage Cray X1 processors kept track of 512 memory operations at once. Just like TRIPS, each of those architectures required extensive compiler support for good performance.</p>

<p>A particularly interesting point is the fully partitioned L1 cache - meaning that there are multiple distributed L1 caches on the chip, so where your instructions are physically executing will be important for performance - if they're near the cache bank holding their operands, they will execute sooner.</p>

<p>The natural question when looking at a new and interesting architecture like this, especially one that promises a tera-op on a chip, is whether it will make its way to a laptop you can buy anytime soon. I have no idea if the UT team has any industry deals in the works, but I would bet against something like this becoming mainstream quickly - the fact that these architectures rely so much on a custom compiler with aggressive optimization means that a lot of dirty work is required to move existing software to it.</p>

<p>It will be interesting to follow this project and see how their actual hardware performs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fran Allen to receive Turing Award]]></title>
    <link href="http://mikemccracken.github.com/blog/2007/02/23/fran-allen-to-receive-turing-award/"/>
    <updated>2007-02-23T04:25:04-08:00</updated>
    <id>http://mikemccracken.github.com/blog/2007/02/23/fran-allen-to-receive-turing-award</id>
    <content type="html"><![CDATA[<p>This is really cool: Fran Allen, a founder of the field of program optimization and compiler analysis, will be the first woman to receive the Turing Award. More info, including a description of her accomplishments, is at the <a href="http://campus.acm.org/public/pressroom/press_releases/2_2007/turing2006.cfm">ACM press release</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing LENS]]></title>
    <link href="http://mikemccracken.github.com/blog/2006/02/14/introducing-lens/"/>
    <updated>2006-02-14T12:29:10-08:00</updated>
    <id>http://mikemccracken.github.com/blog/2006/02/14/introducing-lens</id>
    <content type="html"><![CDATA[<p>I've just put up the website for my current project (to be part of my Ph.D. dissertation work): <a href="http://www.cse.ucsd.edu/~mmccrack/lens/">LENS</a>, a framework for program information manipulation that presents a uniform interface to selective user and automated queries about many types of program metrics, including success and diagnostic information about compiler optimizations and code generation.</p>

<p>I'm not sure how many readers of my weblog will be interested, but there's a link to a technical report on there if you want the gory details.</p>

<p>Feedback and questions are very welcome - the more opportunities I get to explain what I'm doing, the better I get at it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PLDI Papers I'm interested in, part one]]></title>
    <link href="http://mikemccracken.github.com/blog/2006/01/26/pldi-papers-im-interested-in-part-one/"/>
    <updated>2006-01-26T09:39:34-08:00</updated>
    <id>http://mikemccracken.github.com/blog/2006/01/26/pldi-papers-im-interested-in-part-one</id>
    <content type="html"><![CDATA[<p>I mentioned that I'd post about some of the papers I found interesting from this year's PLDI conference. Disclaimer: for the most part this is based on reading the abstracts only, so this shouldn't be considered a thorough review.</p>

<p>Session one is Transactions. I will probably look through these, especially the first paper, <em>"The Atomos Transactional Programming Language"</em> [1] from Stanford, because transactional memory and processing seems to be a consensus pick for the next big thing, and <a href="http://www.computerhistory.org/events/lectures/smith_01232001/">Burton Smith</a> once told me that languages using transactional memory and invariants with respect to state are his bet for what can solve the parallel programming problem. (What problem? It's too hard to write good parallel code.) So, I want to see what a transactional language looks like.</p>

<p>There's a paper in the Compilers session that looks like a cool idea for improving analysis - <em>"A Framework for Unrestricted Whole-Program Optimization"</em> [2]. The <a href="http://liberty.princeton.edu/Publications/index.php?abs=1&amp;setselect;=pldi06_pbe">abstract</a> says they have a way for intra-procedural passes to work on arbitrary subgraphs of the program, so they're not just limited by procedural boundaries, and don't have to rely on inlining to optimize across calls. I'm curious what languages it supports, and how the scheme would work with dynamic languages.</p>

<p>A paper about dynamic software updating, <em>"Practical Dynamic Software updating for C"</em> [3] (<a href="http://www.cs.umd.edu/projects/dsu/">project link</a>) is also interesting, because it seems like a step towards the way things should work. Essentially, they compile a program so that it can be easily updated without stopping it. They do it in a way that doesn't violate type-safety and sounds reasonably efficient. It reminds me of Apple's ZeroLink and Fix &amp; Continue (note that those aren't the first examples of such technology), and I'm curious how similar it is. Certainly I don't think Fix &amp; Continue tries to guarantee type-safety.</p>

<p>The parallelism session should be interesting, and I'm most curious to see an abstract for <em>"Shared Memory Programming for Large Scale Machines"</em> [4], I can't tell from the title if they are introducing a new language or measuring an existing technique. I have a note to myself somewhere to look for a full copy of that paper.</p>

<p>Power has been a big deal in HPC and mobile devices for a while, and now it's everyone's problem, so <em>"Reducing NoC Energy Consumption Through Compiler-Directed Channel Voltage Scaling"</em> [5] caught my eye. I'm always interested to learn about power usage effects of different kinds of code, since I have found it to be satisfyingly unintuitive at times. (Maybe I should've taken more EE classes!) Also, this is a paper from Penn State, and I'm curious what research they've got going on back at my alma mater.</p>

<p>I'll probably read everything in the Runtime Optimization and Profiling session, but <em>"Online Performance Auditing: Using Hot Optimizations Without Getting Burned"</em> [6] is particularly interesting, since I know Brad Calder and his students do really good work, and I honestly didn't know what Jeremy was up to. I should probably be more social around the department. (These guys are at UCSD)</p>

<p>OK, I'm not out of interesting papers, but I'm going to stop here for now. Check out the program, let me know what you think is cool - am I missing something really great?</p>

<h4>References</h4>

<p>[1] "The Atomos Transactional Programming Language"
Brian D. Carlstrom, JaeWoong Chung, Austen McDonald, Hassan Chafi,
Christos Kozyrakis and Kunle Olukotun.</p>

<p>[2] "A Framework for Unrestricted Whole-Program Optimization"
Spyridon Triantafyllis, Matthew J. Bridges, Easwaran Raman, Guilherme Ottoni, and David I. August</p>

<p>[3] "Practical Dynamic Software Updating for C"
Iulian Neamtiu, Michael Hicks, Gareth Stoyle and Manuel Oriol</p>

<p>[4] "Shared Memory Programming for Large Scale Machines"
Christopher Barton, Calin Cascaval, Siddhartha Chatterjee, George Almasi, Yili Zheng, Montse Farreras, Jose Amaral</p>

<p>[5] "Reducing NoC Energy Consumption Through Compiler-Directed Channel Voltage Scaling"
Guangyu Chen, Feihui Li, Mahmut Kandemir, Mary Irwin</p>

<p>[6] "Online Performance Auditing: Using Hot Optimizations Without Getting Burned"
Jeremy Lau, Matthew Arnold, Michael Hind, Brad Calder</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PLDI 2006 Papers]]></title>
    <link href="http://mikemccracken.github.com/blog/2006/01/23/pldi-2006-papers/"/>
    <updated>2006-01-23T10:46:44-08:00</updated>
    <id>http://mikemccracken.github.com/blog/2006/01/23/pldi-2006-papers</id>
    <content type="html"><![CDATA[<p>The <a href="http://research.microsoft.com/conferences/pldi06/Sessions.htm">technical program</a> for PLDI 2006 is out now - there are certainly a lot of interesting papers in there. I'm looking through them now and will probably comment on a few of the ones I think are cool in another post.</p>

<p>PLDI is traditionally a very competitive conference with an emphasis on experimental results, and this year they received 169 submissions and accepted 36. PLDI stands for "Programming Language Design and Implementation", and covers compilers, languages and runtime systems.</p>

<p>There are some interesting workshops co-located with PLDI this year:
 a Workshop on Transactional Memory Workloads (<a href="http://odin.cs.uiuc.edu/WTW/">WTW</a>), a Workshop on Programming Languages and Analysis for Security (<a href="http://www.cis.upenn.edu/~stevez/plas/plas06.html">PLAS</a>), and
 the first ACM SIGPLAN Workshop on Languages, Compilers, and Hardware Support for Transactional Computing (<a href="http://www.cs.purdue.edu/homes/jv/events/TRANSACT/">TRANSACT</a>). Does it sound like transactional computing is hot these days? Yes it does...</p>

<p>Update: for historical reference, this year's 21% acceptance rate puts it right at the average, according to the ACM's data from 1995-2003.</p>
]]></content>
  </entry>
  
</feed>
